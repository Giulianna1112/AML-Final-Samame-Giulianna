{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2K1Knqif9ppO"
      },
      "source": [
        "# üéì Capstone Project - Advanced Machine Learning\n",
        "## TEC-VIII Programa de Especializaci√≥n en Big Data Analytics aplicada a los Negocios\n",
        "\n",
        "---\n",
        "\n",
        "### üìã Informaci√≥n del Proyecto\n",
        "\n",
        "| Campo | Informaci√≥n |\n",
        "|-------|-------------|\n",
        "| **Nombre del Estudiante** | Giulianna Samame Yzena |\n",
        "| **T√≠tulo del Proyecto** | Representation Learning para la Estandarizaci√≥n del Proceso Comercial Cross-Banca mediante Variational Autoencoders en Entornos CRM |\n",
        "| **Fecha de Entrega** | 19.02.2026 |\n",
        "| **Profesor** | Carlos Mari√±o del Rosario|\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-lIaFwP9ppO"
      },
      "source": [
        "## üìë √çndice\n",
        "\n",
        "1. [Resumen Ejecutivo](#1-resumen-ejecutivo)\n",
        "2. [Configuraci√≥n del Entorno](#2-configuraci√≥n-del-entorno)\n",
        "3. [Definici√≥n del Problema de Negocio](#3-definici√≥n-del-problema-de-negocio)\n",
        "4. [Carga y Exploraci√≥n de Datos](#4-carga-y-exploraci√≥n-de-datos)\n",
        "5. [Preprocesamiento de Datos](#5-preprocesamiento-de-datos)\n",
        "6. [Dise√±o y Arquitectura del Modelo](#6-dise√±o-y-arquitectura-del-modelo)\n",
        "7. [Entrenamiento del Modelo](#7-entrenamiento-del-modelo)\n",
        "8. [Evaluaci√≥n y M√©tricas](#8-evaluaci√≥n-y-m√©tricas)\n",
        "9. [Interpretaci√≥n de Resultados](#9-interpretaci√≥n-de-resultados)\n",
        "10. [Conclusiones y Recomendaciones de Negocio](#10-conclusiones-y-recomendaciones-de-negocio)\n",
        "11. [Referencias](#11-referencias)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6FuV50g9ppP"
      },
      "source": [
        "---\n",
        "## 1. Resumen Ejecutivo\n",
        "\n",
        "**Instrucciones:** Proporcione un resumen conciso (m√°ximo 300 palabras) que incluya:\n",
        "- Problema de negocio abordado\n",
        "- Metodolog√≠a utilizada\n",
        "- Principales hallazgos\n",
        "- Impacto esperado en el negocio\n",
        "\n",
        "---\n",
        "## 1. Resumen Ejecutivo\n",
        "\n",
        "En el contexto de la adopci√≥n progresiva del CRM Salesforce en el Banco de Cr√©dito del Per√∫ (BCP), se ha logrado centralizar la gesti√≥n comercial en una √∫nica plataforma de atenci√≥n al cliente. Sin embargo, durante el proceso de migraci√≥n desde sistemas legacy hacia Salesforce, cada banca o l√≠nea de producto ha mantenido definiciones propias de su proceso de venta, utilizando nomenclaturas y criterios distintos para las fases del embudo comercial. Esta heterogeneidad en las etapas del proceso limita la comparabilidad entre unidades de negocio, dificulta la consolidaci√≥n de indicadores comerciales y representa un desaf√≠o estructural para el desarrollo de modelos avanzados de anal√≠tica y Machine Learning.\n",
        "\n",
        "Con el objetivo de abordar esta problem√°tica, se propone una metodolog√≠a basada en t√©cnicas de Representation Learning mediante el uso de redes neuronales profundas, espec√≠ficamente Variational Autoencoders (VAE). Este enfoque permite aprender una representaci√≥n latente com√∫n del proceso comercial a partir de variables como la secuencia de fases, tiempo por etapa, canal de atenci√≥n y n√∫mero de interacciones, reduciendo la dimensionalidad del dataset y eliminando dependencias directas de las definiciones espec√≠ficas de cada banca.\n",
        "\n",
        "Sobre el espacio latente generado, se aplica el algoritmo de clustering K-Means para identificar patrones estructurales en el comportamiento comercial, tales como oportunidades con alta probabilidad de conversi√≥n, procesos con fricci√≥n intermedia o casos de abandono temprano. Adicionalmente, se entrena un modelo de clasificaci√≥n supervisado para estimar la probabilidad de cierre exitoso de una oportunidad comercial.\n",
        "\n",
        "Los principales hallazgos permiten establecer una base anal√≠tica estandarizada cross-banca, facilitando la identificaci√≥n de cuellos de botella en el proceso de venta y mejorando la asignaci√≥n de leads. Se espera que la implementaci√≥n de este modelo contribuya a optimizar la gesti√≥n comercial, reducir sesgos derivados de definiciones heterog√©neas y habilitar soluciones de anal√≠tica avanzada en tiempo real dentro del ecosistema CRM del banco.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1k1xOKO9ppP"
      },
      "source": [
        "## 2. Configuraci√≥n del Entorno\n",
        "\n",
        "### 2.1 Verificaci√≥n de GPU (Recomendado para Deep Learning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rY7XAxJa9ppP"
      },
      "outputs": [],
      "source": [
        "# Verificar si hay GPU disponible\n",
        "import torch\n",
        "\n",
        "# Verificar disponibilidad de GPU\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"‚úÖ GPU disponible: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Memoria GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è GPU no disponible. Usando CPU.\")\n",
        "    print(\"   Recomendaci√≥n: En Colab, vaya a Runtime > Change runtime type > GPU\")\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "print(f\"\\nDispositivo seleccionado: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTL8YBCd9ppP"
      },
      "source": [
        "### 2.2 Instalaci√≥n de Librer√≠as Adicionales (si es necesario)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjUfh5sC9ppP"
      },
      "outputs": [],
      "source": [
        "# Descomente e instale las librer√≠as adicionales que necesite\n",
        "# !pip install transformers\n",
        "# !pip install pytorch-lightning\n",
        "# !pip install optuna\n",
        "# !pip install shap\n",
        "# !pip install lime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9MAObIC9ppP"
      },
      "source": [
        "### 2.3 Importaci√≥n de Librer√≠as"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bao4X6nK9ppP"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# LIBRER√çAS FUNDAMENTALES\n",
        "# =====================================================\n",
        "\n",
        "# Manipulaci√≥n de datos\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Visualizaci√≥n\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Deep Learning - PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
        "\n",
        "# Deep Learning - TensorFlow/Keras (alternativa)\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "\n",
        "# Preprocesamiento\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                             f1_score, confusion_matrix, classification_report,\n",
        "                             mean_squared_error, mean_absolute_error, r2_score)\n",
        "\n",
        "# Utilidades\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuraci√≥n de visualizaci√≥n\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette('husl')\n",
        "%matplotlib inline\n",
        "\n",
        "# Semilla para reproducibilidad\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "tf.random.set_seed(RANDOM_SEED)\n",
        "\n",
        "print(\"‚úÖ Todas las librer√≠as importadas correctamente\")\n",
        "print(f\"   PyTorch version: {torch.__version__}\")\n",
        "print(f\"   TensorFlow version: {tf.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plOIZFi49ppQ"
      },
      "source": [
        "### 2.4 Conexi√≥n con Google Drive (para cargar datos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mgj-Kekq9ppQ"
      },
      "outputs": [],
      "source": [
        "# Montar Google Drive para acceder a los datos\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Definir la ruta base de su proyecto\n",
        "# Modifique esta ruta seg√∫n la ubicaci√≥n de sus datos\n",
        "BASE_PATH = '/content/drive/MyDrive/Capstone_Project/'\n",
        "\n",
        "print(f\"‚úÖ Google Drive montado\")\n",
        "print(f\"   Ruta base del proyecto: {BASE_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOtBYWOq9ppQ"
      },
      "source": [
        "---\n",
        "## 3. Definici√≥n del Problema de Negocio\n",
        "\n",
        "### 3.1 Contexto del Negocio\n",
        "\n",
        "**Instrucciones:** Describa el contexto empresarial, incluyendo:\n",
        "- Industria/Sector\n",
        "- Empresa o caso de estudio\n",
        "- Situaci√≥n actual\n",
        "\n",
        "---\n",
        "El presente proyecto se desarrolla en el contexto del sector financiero, espec√≠ficamente dentro de la industria de banca comercial, caracterizada por una alta intensidad en el uso de datos para la gesti√≥n de clientes, productos financieros y procesos de venta. En este entorno, la digitalizaci√≥n de los canales de atenci√≥n y la automatizaci√≥n de los procesos comerciales constituyen factores cr√≠ticos para mejorar la eficiencia operativa y la experiencia del cliente en agencias.\n",
        "\n",
        "El caso de estudio corresponde al Banco de Cr√©dito del Per√∫ (BCP), entidad que desde el a√±o 2019 ha venido adoptando progresivamente la plataforma CRM Salesforce como herramienta principal para la gesti√≥n comercial de sus productos financieros. Previamente, la atenci√≥n al cliente en agencias se realizaba mediante m√∫ltiples sistemas legacy desarrollados internamente, donde cada equipo de negocio operaba aplicativos distintos para la ejecuci√≥n de transacciones como la apertura de cuentas de ahorro, otorgamiento de pr√©stamos o validaci√≥n de datos del cliente. Esta arquitectura fragmentada generaba tiempos prolongados de atenci√≥n, duplicidad de esfuerzos operativos y elevados costos de mantenimiento asociados a la actualizaci√≥n y soporte de dichos sistemas.\n",
        "\n",
        "En la actualidad, Salesforce opera como plataforma unificada de interacci√≥n comercial (front-end), registrando las actividades realizadas por los asesores durante el proceso de atenci√≥n al cliente. Sin embargo, los datos transaccionales y operativos generados son posteriormente descargados hacia el Data Lake corporativo para su explotaci√≥n anal√≠tica. Durante el proceso de migraci√≥n desde los sistemas legacy hacia Salesforce, cada banca o l√≠nea de producto mantuvo definiciones propias de su flujo de venta, estableciendo fases comerciales con nomenclaturas y criterios distintos dentro del embudo de conversi√≥n.\n",
        "\n",
        "Esta situaci√≥n ha derivado en una falta de estandarizaci√≥n en las etapas del proceso comercial entre unidades de negocio, dificultando la consolidaci√≥n de informaci√≥n, la comparabilidad de indicadores de desempe√±o y el desarrollo de modelos avanzados de anal√≠tica predictiva sobre el proceso de ventas. En consecuencia, la organizaci√≥n enfrenta actualmente limitaciones estructurales para implementar soluciones de Machine Learning que permitan optimizar la gesti√≥n de oportunidades comerciales y mejorar la toma de decisiones basada en datos.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 3.2 Problema a Resolver\n",
        "\n",
        "**Instrucciones:** Defina claramente:\n",
        "- ¬øCu√°l es el problema espec√≠fico?\n",
        "- ¬øPor qu√© es importante resolverlo?\n",
        "- ¬øCu√°l es el impacto actual del problema?\n",
        "\n",
        "---\n",
        "\n",
        "El problema espec√≠fico radica en la falta de estandarizaci√≥n de las fases del proceso comercial entre las distintas bancas o l√≠neas de producto dentro del CRM Salesforce. Durante el proceso de migraci√≥n desde sistemas legacy hacia la plataforma actual, cada unidad de negocio mantuvo definiciones propias de su flujo de venta, asignando nomenclaturas y criterios distintos a las etapas del embudo comercial. Como resultado, oportunidades que se encuentran en estados funcionalmente equivalentes pueden ser registradas bajo denominaciones diferentes dependiendo de la banca que las gestione.\n",
        "\n",
        "Resolver esta problem√°tica es fundamental debido a que la heterogeneidad en la definici√≥n de las etapas del proceso comercial impide la consolidaci√≥n de informaci√≥n a nivel organizacional, limita la comparabilidad entre indicadores de desempe√±o comercial y dificulta la construcci√≥n de variables consistentes para el desarrollo de modelos anal√≠ticos. En particular, esta situaci√≥n representa una barrera significativa para la implementaci√≥n de soluciones de Machine Learning orientadas a la predicci√≥n de cierre de oportunidades o a la optimizaci√≥n del proceso de venta, dado que los modelos dependen de representaciones homog√©neas del comportamiento comercial.\n",
        "\n",
        "El impacto actual del problema se manifiesta en la imposibilidad de analizar de manera transversal el desempe√±o del proceso de ventas entre bancas, identificar cuellos de botella comunes o establecer estrategias de asignaci√≥n de leads basadas en patrones hist√≥ricos de conversi√≥n. Asimismo, la falta de estandarizaci√≥n introduce sesgos en los an√°lisis comerciales y reduce la capacidad de la organizaci√≥n para implementar iniciativas de anal√≠tica avanzada en tiempo real, afectando potencialmente la eficiencia operativa en agencias y la efectividad de la gesti√≥n comercial.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 3.3 Objetivos del Proyecto\n",
        "\n",
        "**Instrucciones:** Liste los objetivos SMART (Espec√≠ficos, Medibles, Alcanzables, Relevantes, Temporales)\n",
        "\n",
        "---\n",
        "\n",
        "**Objetivo General:**\n",
        "\n",
        "Desarrollar un modelo basado en t√©cnicas de Representation Learning mediante Variational Autoencoders (VAE) que permita generar una representaci√≥n latente estandarizada del proceso comercial en el CRM Salesforce, con el fin de mejorar la comparabilidad de indicadores entre bancas y habilitar la implementaci√≥n de modelos predictivos para la estimaci√≥n de la probabilidad de cierre de oportunidades comerciales en un plazo de 6 meses.\n",
        "\n",
        "**Objetivos Espec√≠ficos:**\n",
        "\n",
        "1. Identificar y estructurar variables representativas del proceso comercial (fases del embudo, tiempo por etapa, canal de atenci√≥n y n√∫mero de interacciones) a partir de los registros generados en Salesforce y almacenados en el Data Lake corporativo, en un periodo m√°ximo de 2 meses.\n",
        "\n",
        "2. Dise√±ar y entrenar un modelo de Variational Autoencoder (VAE) que permita reducir la dimensionalidad del proceso de ventas y generar un espacio latente com√∫n para oportunidades comerciales provenientes de distintas bancas, logrando una reducci√≥n m√≠nima del 30% en la variabilidad asociada a definiciones heterog√©neas de fases en un plazo de 4 meses.\n",
        "\n",
        "3. Implementar un modelo de clasificaci√≥n supervisado sobre el espacio latente generado que permita estimar la probabilidad de cierre exitoso de oportunidades comerciales con una precisi√≥n m√≠nima del 75%, contribuyendo a la mejora en la asignaci√≥n de leads y priorizaci√≥n de gestiones comerciales en un plazo de 6 meses.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 3.4 Tipo de Problema de Machine Learning\n",
        "\n",
        "**Instrucciones:** Identifique el tipo de problema:\n",
        "- [x ] Clasificaci√≥n binaria\n",
        "- [ ] Clasificaci√≥n multiclase\n",
        "- [ ] Regresi√≥n\n",
        "- [x ] Clustering\n",
        "- [ ] Series temporales\n",
        "- [ ] Procesamiento de Lenguaje Natural (NLP)\n",
        "- [ ] Visi√≥n por Computadora\n",
        "- [ ] Otro: _________\n",
        "\n",
        "**Justificaci√≥n:**\n",
        "El problema central de negocio consiste en estimar la probabilidad de cierre exitoso de una oportunidad comercial dentro del CRM Salesforce. Dado que el resultado de inter√©s tiene dos posibles estados (cierre exitoso vs. no cierre o abandono), el problema principal se enmarca como una tarea de clasificaci√≥n binaria supervisada.\n",
        "\n",
        "No obstante, previo al entrenamiento del modelo predictivo, se requiere abordar la heterogeneidad estructural en las fases del proceso comercial entre distintas bancas. Para ello, se emplea una etapa de aprendizaje no supervisado mediante Variational Autoencoders (VAE), t√©cnica de Representation Learning que permite generar un espacio latente com√∫n independiente de las nomenclaturas espec√≠ficas de cada flujo comercial.\n",
        "\n",
        "Sobre dicho espacio latente se aplica adicionalmente un algoritmo de clustering (K-Means) con el objetivo de identificar patrones estructurales en el comportamiento de las oportunidades comerciales, tales como procesos con alta fricci√≥n o alta probabilidad de conversi√≥n.\n",
        "\n",
        "En consecuencia, el proyecto combina t√©cnicas no supervisadas (Representation Learning y Clustering) con un modelo supervisado de clasificaci√≥n binaria, siendo este √∫ltimo el que responde directamente al objetivo principal de negocio: mejorar la priorizaci√≥n de oportunidades comerciales y optimizar la asignaci√≥n de leads.\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciTDVjCS9ppQ"
      },
      "source": [
        "---\n",
        "## 4. Carga y Exploraci√≥n de Datos\n",
        "\n",
        "### 4.1 Carga de Datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_zhNwkk9ppQ"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# CARGA DE DATOS\n",
        "# =====================================================\n",
        "\n",
        "# Opci√≥n 1: Cargar desde Google Drive\n",
        "# df = pd.read_csv(BASE_PATH + 'datos.csv')\n",
        "\n",
        "# Opci√≥n 2: Cargar desde URL\n",
        "# df = pd.read_csv('https://url-de-sus-datos.com/datos.csv')\n",
        "\n",
        "# Opci√≥n 3: Cargar desde archivo local (subido a Colab)\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "# df = pd.read_csv('nombre_archivo.csv')\n",
        "\n",
        "# Opci√≥n 4: Dataset de ejemplo (para testing)\n",
        "# from sklearn.datasets import load_iris, load_boston, fetch_california_housing\n",
        "# data = load_iris()\n",
        "# df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "# df['target'] = data.target\n",
        "\n",
        "# =====================================================\n",
        "# COMPLETE AQU√ç: Cargue su dataset\n",
        "# =====================================================\n",
        "\n",
        "# df = pd.read_csv('...')  # Descomente y complete\n",
        "\n",
        "print(f\"‚úÖ Dataset cargado exitosamente\")\n",
        "print(f\"   Dimensiones: {df.shape[0]:,} filas √ó {df.shape[1]} columnas\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDdBpOH39ppQ"
      },
      "source": [
        "### 4.2 Descripci√≥n del Dataset\n",
        "\n",
        "**Instrucciones:** Describa su dataset:\n",
        "- Fuente de los datos\n",
        "- Per√≠odo de tiempo que cubren\n",
        "- Descripci√≥n de cada variable\n",
        "\n",
        "\n",
        "\n",
        "El dataset utilizado en el presente proyecto proviene de los registros operativos generados en el CRM Salesforce, espec√≠ficamente de las interacciones realizadas por los asesores comerciales durante el proceso de atenci√≥n a clientes en agencias. Esta informaci√≥n es posteriormente almacenada y procesada en el Data Lake corporativo del Banco de Cr√©dito del Per√∫ (BCP), desde donde es consumida para fines anal√≠ticos.\n",
        "\n",
        "El conjunto de datos cubre un per√≠odo de 12 meses comprendido entre enero de 2025 y diciembre de 2025, e incluye informaci√≥n relacionada al comportamiento de oportunidades comerciales a lo largo del embudo de ventas, considerando variables operativas asociadas a las distintas fases del proceso comercial.\n",
        "\n",
        "A continuaci√≥n, se presenta la descripci√≥n de las principales variables consideradas en el modelo:\n",
        "\n",
        "| Variable | Tipo | Descripci√≥n |\n",
        "|----------|------|-------------|\n",
        "| fase_comercial | categ√≥rica | Etapa del proceso de venta en la que se encuentra la oportunidad comercial |\n",
        "| tiempo_en_fase | num√©rica | Tiempo (en d√≠as) que la oportunidad permanece en una fase espec√≠fica del embudo |\n",
        "| canal_atencion | categ√≥rica | Canal utilizado para la gesti√≥n de la oportunidad (agencia, telef√≥nico, digital) |\n",
        "| num_interacciones | num√©rica | N√∫mero total de interacciones realizadas con el cliente durante el proceso comercial |\n",
        "| tipo_producto | categ√≥rica | Categor√≠a del producto financiero asociado a la oportunidad (pr√©stamo, cuenta, tarjeta, etc.) |\n",
        "| segmento_cliente | categ√≥rica | Segmento al que pertenece el cliente seg√∫n clasificaci√≥n interna |\n",
        "| asesor_id | categ√≥rica | Identificador del asesor que gestiona la oportunidad |\n",
        "| fecha_creacion | temporal | Fecha de creaci√≥n de la oportunidad comercial en Salesforce |\n",
        "| fecha_cierre | temporal | Fecha en la que se cerr√≥ la oportunidad comercial |\n",
        "| estado_oportunidad | categ√≥rica | Estado final de la oportunidad (cerrada ganada, cerrada perdida) |\n",
        "| target | binaria | Variable objetivo que indica si la oportunidad fue cerrada exitosamente (1) o no (0) |\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTSIxytu9ppQ"
      },
      "source": [
        "### 4.3 Exploraci√≥n Inicial de Datos (EDA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67yyfdQ59ppQ"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# INFORMACI√ìN GENERAL DEL DATASET\n",
        "# =====================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"INFORMACI√ìN GENERAL DEL DATASET\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Primeras filas\n",
        "print(\"\\nüìä Primeras 5 filas:\")\n",
        "display(df.head())\n",
        "\n",
        "# Informaci√≥n del dataset\n",
        "print(\"\\nüìã Informaci√≥n del Dataset:\")\n",
        "print(df.info())\n",
        "\n",
        "# Estad√≠sticas descriptivas\n",
        "print(\"\\nüìà Estad√≠sticas Descriptivas:\")\n",
        "display(df.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26lmisNK9ppQ"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# AN√ÅLISIS DE VALORES FALTANTES\n",
        "# =====================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"AN√ÅLISIS DE VALORES FALTANTES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Calcular valores faltantes\n",
        "missing_data = pd.DataFrame({\n",
        "    'Total Faltantes': df.isnull().sum(),\n",
        "    'Porcentaje (%)': (df.isnull().sum() / len(df) * 100).round(2)\n",
        "})\n",
        "missing_data = missing_data[missing_data['Total Faltantes'] > 0].sort_values('Porcentaje (%)', ascending=False)\n",
        "\n",
        "if len(missing_data) > 0:\n",
        "    print(\"\\n‚ö†Ô∏è Variables con valores faltantes:\")\n",
        "    display(missing_data)\n",
        "\n",
        "    # Visualizaci√≥n de valores faltantes\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x=missing_data.index, y='Porcentaje (%)', data=missing_data)\n",
        "    plt.title('Porcentaje de Valores Faltantes por Variable')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.ylabel('Porcentaje (%)')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"\\n‚úÖ No hay valores faltantes en el dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K91EI_4b9ppQ"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# AN√ÅLISIS DE LA VARIABLE OBJETIVO\n",
        "# =====================================================\n",
        "\n",
        "# COMPLETE: Especifique el nombre de su variable objetivo\n",
        "TARGET_COLUMN = 'target'  # Cambie 'target' por el nombre de su variable objetivo\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(f\"AN√ÅLISIS DE LA VARIABLE OBJETIVO: {TARGET_COLUMN}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Para clasificaci√≥n\n",
        "if df[TARGET_COLUMN].dtype == 'object' or df[TARGET_COLUMN].nunique() < 20:\n",
        "    print(\"\\nüìä Distribuci√≥n de clases:\")\n",
        "    class_dist = df[TARGET_COLUMN].value_counts()\n",
        "    print(class_dist)\n",
        "\n",
        "    # Visualizaci√≥n\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Gr√°fico de barras\n",
        "    sns.countplot(data=df, x=TARGET_COLUMN, ax=axes[0])\n",
        "    axes[0].set_title(f'Distribuci√≥n de {TARGET_COLUMN}')\n",
        "    axes[0].set_xlabel(TARGET_COLUMN)\n",
        "    axes[0].set_ylabel('Frecuencia')\n",
        "\n",
        "    # Gr√°fico de pastel\n",
        "    axes[1].pie(class_dist.values, labels=class_dist.index, autopct='%1.1f%%', startangle=90)\n",
        "    axes[1].set_title(f'Proporci√≥n de {TARGET_COLUMN}')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Verificar desbalance\n",
        "    imbalance_ratio = class_dist.max() / class_dist.min()\n",
        "    if imbalance_ratio > 3:\n",
        "        print(f\"\\n‚ö†Ô∏è ADVERTENCIA: Dataset desbalanceado (ratio {imbalance_ratio:.2f}:1)\")\n",
        "        print(\"   Considere t√©cnicas de balanceo: SMOTE, undersampling, class weights\")\n",
        "else:\n",
        "    # Para regresi√≥n\n",
        "    print(\"\\nüìä Estad√≠sticas de la variable objetivo:\")\n",
        "    print(df[TARGET_COLUMN].describe())\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Histograma\n",
        "    sns.histplot(df[TARGET_COLUMN], kde=True, ax=axes[0])\n",
        "    axes[0].set_title(f'Distribuci√≥n de {TARGET_COLUMN}')\n",
        "\n",
        "    # Box plot\n",
        "    sns.boxplot(y=df[TARGET_COLUMN], ax=axes[1])\n",
        "    axes[1].set_title(f'Box Plot de {TARGET_COLUMN}')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJrwb4og9ppQ"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# AN√ÅLISIS DE CORRELACIONES\n",
        "# =====================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"MATRIZ DE CORRELACIONES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Seleccionar solo columnas num√©ricas\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "if len(numeric_cols) > 1:\n",
        "    # Calcular correlaciones\n",
        "    correlation_matrix = df[numeric_cols].corr()\n",
        "\n",
        "    # Visualizaci√≥n\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
        "    sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm',\n",
        "                center=0, fmt='.2f', linewidths=0.5)\n",
        "    plt.title('Matriz de Correlaciones')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Correlaciones con la variable objetivo\n",
        "    if TARGET_COLUMN in numeric_cols:\n",
        "        print(f\"\\nüìä Correlaciones con {TARGET_COLUMN}:\")\n",
        "        target_corr = correlation_matrix[TARGET_COLUMN].drop(TARGET_COLUMN).sort_values(ascending=False)\n",
        "        print(target_corr)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No hay suficientes columnas num√©ricas para an√°lisis de correlaci√≥n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6_AxGx99ppQ"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# VISUALIZACIONES ADICIONALES\n",
        "# =====================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"VISUALIZACIONES ADICIONALES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Distribuci√≥n de variables num√©ricas\n",
        "numeric_cols_plot = df.select_dtypes(include=[np.number]).columns[:8]  # Primeras 8 columnas\n",
        "\n",
        "if len(numeric_cols_plot) > 0:\n",
        "    n_cols = 2\n",
        "    n_rows = (len(numeric_cols_plot) + 1) // 2\n",
        "\n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 4*n_rows))\n",
        "    axes = axes.flatten() if n_rows > 1 else [axes]\n",
        "\n",
        "    for i, col in enumerate(numeric_cols_plot):\n",
        "        if i < len(axes):\n",
        "            sns.histplot(df[col], kde=True, ax=axes[i])\n",
        "            axes[i].set_title(f'Distribuci√≥n de {col}')\n",
        "\n",
        "    # Ocultar ejes vac√≠os\n",
        "    for j in range(i+1, len(axes)):\n",
        "        axes[j].set_visible(False)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKvVh9wy9ppQ"
      },
      "source": [
        "### 4.4 Hallazgos del EDA\n",
        "\n",
        "**Instrucciones:** Resuma los principales hallazgos de la exploraci√≥n de datos:\n",
        "\n",
        "---\n",
        "\n",
        "**Hallazgos Principales:**\n",
        "1. Se identific√≥ que las oportunidades comerciales presentan una alta variabilidad en la duraci√≥n de las fases del embudo de ventas, evidenciando diferencias significativas en el tiempo promedio de permanencia seg√∫n la banca o l√≠nea de producto.\n",
        "\n",
        "2. Se observ√≥ que la cantidad de interacciones con el cliente var√≠a considerablemente entre oportunidades que alcanzan el cierre exitoso y aquellas que permanecen estancadas en fases intermedias del proceso comercial.\n",
        "\n",
        "3. Se evidenci√≥ que determinadas combinaciones de canal de atenci√≥n y tipo de producto est√°n asociadas a mayores tasas de conversi√≥n, sugiriendo la existencia de patrones estructurales en el comportamiento del proceso de ventas.\n",
        "\n",
        "**Problemas Identificados:**\n",
        "1. La nomenclatura de las fases comerciales no es homog√©nea entre las distintas bancas, lo que dificulta la comparaci√≥n directa de oportunidades en estados funcionalmente equivalentes dentro del embudo de ventas.\n",
        "\n",
        "2. La heterogeneidad en las definiciones del proceso comercial introduce ruido estructural en el dataset, afectando la consistencia de las variables categ√≥ricas y limitando la capacidad de los modelos supervisados para capturar patrones de conversi√≥n.\n",
        "\n",
        "**Acciones a Tomar:**\n",
        "1. Aplicar t√©cnicas de Representation Learning mediante Variational Autoencoders (VAE) para generar un espacio latente que capture la din√°mica del proceso comercial de forma independiente a la nomenclatura espec√≠fica de cada banca.\n",
        "\n",
        "2. Estandarizar las variables categ√≥ricas del proceso de venta a trav√©s de su transformaci√≥n en representaciones vectoriales latentes, con el fin de mejorar la calidad del dataset y habilitar el entrenamiento de modelos predictivos de clasificaci√≥n binaria.\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijEGwPaN9ppQ"
      },
      "source": [
        "---\n",
        "## 5. Preprocesamiento de Datos\n",
        "\n",
        "### 5.1 Tratamiento de Valores Faltantes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHaqilDU9ppR"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# TRATAMIENTO DE VALORES FALTANTES\n",
        "# =====================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"TRATAMIENTO DE VALORES FALTANTES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Crear copia del dataframe\n",
        "df_clean = df.copy()\n",
        "\n",
        "# Opci√≥n 1: Eliminar filas con valores faltantes\n",
        "# df_clean = df_clean.dropna()\n",
        "\n",
        "# Opci√≥n 2: Imputar con la media (variables num√©ricas)\n",
        "# from sklearn.impute import SimpleImputer\n",
        "# imputer = SimpleImputer(strategy='mean')\n",
        "# df_clean[numeric_cols] = imputer.fit_transform(df_clean[numeric_cols])\n",
        "\n",
        "# Opci√≥n 3: Imputar con la moda (variables categ√≥ricas)\n",
        "# for col in categorical_cols:\n",
        "#     df_clean[col].fillna(df_clean[col].mode()[0], inplace=True)\n",
        "\n",
        "# Opci√≥n 4: Imputaci√≥n avanzada con KNN\n",
        "# from sklearn.impute import KNNImputer\n",
        "# imputer = KNNImputer(n_neighbors=5)\n",
        "# df_clean[numeric_cols] = imputer.fit_transform(df_clean[numeric_cols])\n",
        "\n",
        "# =====================================================\n",
        "# COMPLETE AQU√ç: Aplique su estrategia de imputaci√≥n\n",
        "\n",
        "# ‚úÖ Estrategia elegida: imputaci√≥n simple (num√©ricas = media, categ√≥ricas = moda)\n",
        "\n",
        "#from sklearn.impute import SimpleImputer\n",
        "\n",
        "# 1) Imputaci√≥n num√©rica con media\n",
        "#num_imputer = SimpleImputer(strategy=\"mean\")\n",
        "#df_clean[numeric_cols] = num_imputer.fit_transform(df_clean[numeric_cols])\n",
        "\n",
        "# 2) Imputaci√≥n categ√≥rica con moda\n",
        "#cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
        "#df_clean[categorical_cols] = cat_imputer.fit_transform(df_clean[categorical_cols])\n",
        "\n",
        "# =====================================================\n",
        "\n",
        "\n",
        "\n",
        "print(f\"\\n‚úÖ Valores faltantes tratados\")\n",
        "print(f\"   Filas restantes: {len(df_clean):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwHKno2N9ppR"
      },
      "source": [
        "### 5.2 Tratamiento de Outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouq7WaqT9ppR"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# DETECCI√ìN Y TRATAMIENTO DE OUTLIERS\n",
        "# =====================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"DETECCI√ìN DE OUTLIERS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def detect_outliers_iqr(data, column):\n",
        "    \"\"\"Detecta outliers usando el m√©todo IQR\"\"\"\n",
        "    Q1 = data[column].quantile(0.25)\n",
        "    Q3 = data[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
        "    return len(outliers), lower_bound, upper_bound\n",
        "\n",
        "# Detectar outliers en cada columna num√©rica\n",
        "numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "outlier_summary = []\n",
        "for col in numeric_cols:\n",
        "    n_outliers, lower, upper = detect_outliers_iqr(df_clean, col)\n",
        "    if n_outliers > 0:\n",
        "        outlier_summary.append({\n",
        "            'Variable': col,\n",
        "            'N_Outliers': n_outliers,\n",
        "            'Porcentaje (%)': round(n_outliers/len(df_clean)*100, 2),\n",
        "            'L√≠mite_Inferior': round(lower, 2),\n",
        "            'L√≠mite_Superior': round(upper, 2)\n",
        "        })\n",
        "\n",
        "if outlier_summary:\n",
        "    outlier_df = pd.DataFrame(outlier_summary)\n",
        "    print(\"\\n‚ö†Ô∏è Variables con outliers detectados:\")\n",
        "    display(outlier_df)\n",
        "else:\n",
        "    print(\"\\n‚úÖ No se detectaron outliers significativos\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmJRhfq89ppR"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# TRATAMIENTO DE OUTLIERS (OPCIONAL)\n",
        "# =====================================================\n",
        "\n",
        "# Opci√≥n 1: Eliminar outliers\n",
        "# for col in numeric_cols:\n",
        "#     Q1, Q3 = df_clean[col].quantile([0.25, 0.75])\n",
        "#     IQR = Q3 - Q1\n",
        "#     df_clean = df_clean[(df_clean[col] >= Q1 - 1.5*IQR) & (df_clean[col] <= Q3 + 1.5*IQR)]\n",
        "\n",
        "# Opci√≥n 2: Capear outliers (winsorizing)\n",
        "# from scipy.stats import mstats\n",
        "# for col in numeric_cols:\n",
        "#     df_clean[col] = mstats.winsorize(df_clean[col], limits=[0.05, 0.05])\n",
        "\n",
        "# Opci√≥n 3: Transformaci√≥n logar√≠tmica\n",
        "# for col in cols_to_transform:\n",
        "#     df_clean[col] = np.log1p(df_clean[col])\n",
        "\n",
        "# =====================================================\n",
        "# COMPLETE AQU√ç: Aplique su estrategia de tratamiento\n",
        "# =====================================================\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYi0DJlD9ppR"
      },
      "source": [
        "### 5.3 Codificaci√≥n de Variables Categ√≥ricas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IM4hU5xS9ppR"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# CODIFICACI√ìN DE VARIABLES CATEG√ìRICAS\n",
        "# =====================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"CODIFICACI√ìN DE VARIABLES CATEG√ìRICAS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Identificar variables categ√≥ricas\n",
        "categorical_cols = df_clean.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "print(f\"\\nVariables categ√≥ricas encontradas: {categorical_cols}\")\n",
        "\n",
        "# Opci√≥n 1: Label Encoding (para variables ordinales o target)\n",
        "# le = LabelEncoder()\n",
        "# df_clean['columna_encoded'] = le.fit_transform(df_clean['columna'])\n",
        "\n",
        "# Opci√≥n 2: One-Hot Encoding (para variables nominales)\n",
        "# df_clean = pd.get_dummies(df_clean, columns=categorical_cols, drop_first=True)\n",
        "\n",
        "# Opci√≥n 3: Target Encoding\n",
        "# from sklearn.preprocessing import TargetEncoder\n",
        "# encoder = TargetEncoder()\n",
        "# df_clean[categorical_cols] = encoder.fit_transform(df_clean[categorical_cols], df_clean[TARGET_COLUMN])\n",
        "\n",
        "# =====================================================\n",
        "# COMPLETE AQU√ç: Aplique su estrategia de codificaci√≥n\n",
        "# =====================================================\n",
        "# =====================================================\n",
        "# Estrategia seleccionada: Label Encoding\n",
        "# =====================================================\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Crear un diccionario para almacenar los encoders por variable\n",
        "#label_encoders = {}\n",
        "\n",
        "#for col in categorical_cols:\n",
        "    #le = LabelEncoder()\n",
        "    #df_clean[col] = le.fit_transform(df_clean[col].astype(str))\n",
        "    #label_encoders[col] = le\n",
        "\n",
        "#print(\"Variables categ√≥ricas codificadas mediante Label Encoding.\")\n",
        "\n",
        "\n",
        "print(f\"\\n‚úÖ Codificaci√≥n completada\")\n",
        "print(f\"   Dimensiones finales: {df_clean.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VLDrqPY9ppR"
      },
      "source": [
        "### 5.4 Escalado/Normalizaci√≥n de Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X17HMZyz9ppR"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# ESCALADO DE FEATURES\n",
        "# =====================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"ESCALADO DE FEATURES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Separar features y target\n",
        "X = df_clean.drop(columns=[TARGET_COLUMN])\n",
        "y = df_clean[TARGET_COLUMN]\n",
        "\n",
        "print(f\"\\nDimensiones de X: {X.shape}\")\n",
        "print(f\"Dimensiones de y: {y.shape}\")\n",
        "\n",
        "# Opci√≥n 1: StandardScaler (media=0, std=1) - Recomendado para redes neuronales\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Opci√≥n 2: MinMaxScaler (rango [0,1])\n",
        "# scaler = MinMaxScaler()\n",
        "\n",
        "# Opci√≥n 3: RobustScaler (robusto a outliers)\n",
        "# from sklearn.preprocessing import RobustScaler\n",
        "# scaler = RobustScaler()\n",
        "\n",
        "# Aplicar escalado\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_scaled = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
        "\n",
        "print(f\"\\n‚úÖ Escalado completado usando {type(scaler).__name__}\")\n",
        "print(f\"   Media de features: {X_scaled.mean().mean():.6f}\")\n",
        "print(f\"   Std de features: {X_scaled.std().mean():.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqmrqxNt9ppR"
      },
      "source": [
        "### 5.5 Divisi√≥n de Datos (Train/Validation/Test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDcxWPJW9ppR"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# DIVISI√ìN DE DATOS\n",
        "# =====================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"DIVISI√ìN DE DATOS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Divisi√≥n en train (70%), validation (15%), test (15%)\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.15, random_state=RANDOM_SEED, stratify=y if y.dtype == 'object' or y.nunique() < 20 else None\n",
        ")\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.176, random_state=RANDOM_SEED, stratify=y_temp if y_temp.dtype == 'object' or y_temp.nunique() < 20 else None  # 0.176 ‚âà 15% del total\n",
        ")\n",
        "\n",
        "print(f\"\\nüìä Divisi√≥n de datos:\")\n",
        "print(f\"   Training set:   {X_train.shape[0]:,} muestras ({X_train.shape[0]/len(X_scaled)*100:.1f}%)\")\n",
        "print(f\"   Validation set: {X_val.shape[0]:,} muestras ({X_val.shape[0]/len(X_scaled)*100:.1f}%)\")\n",
        "print(f\"   Test set:       {X_test.shape[0]:,} muestras ({X_test.shape[0]/len(X_scaled)*100:.1f}%)\")\n",
        "\n",
        "# Verificar distribuci√≥n de clases (para clasificaci√≥n)\n",
        "if y.dtype == 'object' or y.nunique() < 20:\n",
        "    print(f\"\\nüìä Distribuci√≥n de clases en cada conjunto:\")\n",
        "    print(f\"   Train: {dict(y_train.value_counts(normalize=True).round(3))}\")\n",
        "    print(f\"   Val:   {dict(y_val.value_counts(normalize=True).round(3))}\")\n",
        "    print(f\"   Test:  {dict(y_test.value_counts(normalize=True).round(3))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okF2ahUa9ppR"
      },
      "source": [
        "### 5.6 Preparaci√≥n de Datos para Deep Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwYrAVc99ppR"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# PREPARACI√ìN PARA PYTORCH\n",
        "# =====================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"PREPARACI√ìN DE DATOS PARA PYTORCH\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Convertir a tensores de PyTorch\n",
        "X_train_tensor = torch.FloatTensor(X_train.values)\n",
        "X_val_tensor = torch.FloatTensor(X_val.values)\n",
        "X_test_tensor = torch.FloatTensor(X_test.values)\n",
        "\n",
        "# Para clasificaci√≥n\n",
        "if y.dtype == 'object' or y.nunique() < 20:\n",
        "    # Codificar labels si es necesario\n",
        "    if y_train.dtype == 'object':\n",
        "        label_encoder = LabelEncoder()\n",
        "        y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "        y_val_encoded = label_encoder.transform(y_val)\n",
        "        y_test_encoded = label_encoder.transform(y_test)\n",
        "    else:\n",
        "        y_train_encoded = y_train.values\n",
        "        y_val_encoded = y_val.values\n",
        "        y_test_encoded = y_test.values\n",
        "\n",
        "    y_train_tensor = torch.LongTensor(y_train_encoded)\n",
        "    y_val_tensor = torch.LongTensor(y_val_encoded)\n",
        "    y_test_tensor = torch.LongTensor(y_test_encoded)\n",
        "else:\n",
        "    # Para regresi√≥n\n",
        "    y_train_tensor = torch.FloatTensor(y_train.values).unsqueeze(1)\n",
        "    y_val_tensor = torch.FloatTensor(y_val.values).unsqueeze(1)\n",
        "    y_test_tensor = torch.FloatTensor(y_test.values).unsqueeze(1)\n",
        "\n",
        "# Crear DataLoaders\n",
        "BATCH_SIZE = 32  # Ajuste seg√∫n su dataset\n",
        "\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(f\"\\n‚úÖ DataLoaders creados\")\n",
        "print(f\"   Batch size: {BATCH_SIZE}\")\n",
        "print(f\"   Batches de entrenamiento: {len(train_loader)}\")\n",
        "print(f\"   Batches de validaci√≥n: {len(val_loader)}\")\n",
        "print(f\"   Batches de test: {len(test_loader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4OJW4NMn9ppR"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# PREPARACI√ìN PARA TENSORFLOW/KERAS (ALTERNATIVA)\n",
        "# =====================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"PREPARACI√ìN DE DATOS PARA TENSORFLOW/KERAS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Convertir a arrays numpy (Keras acepta DataFrames directamente, pero es mejor convertir)\n",
        "X_train_np = X_train.values.astype('float32')\n",
        "X_val_np = X_val.values.astype('float32')\n",
        "X_test_np = X_test.values.astype('float32')\n",
        "\n",
        "# Para clasificaci√≥n: One-hot encoding del target\n",
        "if y.dtype == 'object' or y.nunique() < 20:\n",
        "    num_classes = y.nunique()\n",
        "    y_train_np = keras.utils.to_categorical(y_train_encoded, num_classes)\n",
        "    y_val_np = keras.utils.to_categorical(y_val_encoded, num_classes)\n",
        "    y_test_np = keras.utils.to_categorical(y_test_encoded, num_classes)\n",
        "else:\n",
        "    y_train_np = y_train.values.astype('float32')\n",
        "    y_val_np = y_val.values.astype('float32')\n",
        "    y_test_np = y_test.values.astype('float32')\n",
        "\n",
        "print(f\"\\n‚úÖ Datos preparados para TensorFlow/Keras\")\n",
        "print(f\"   Shape X_train: {X_train_np.shape}\")\n",
        "print(f\"   Shape y_train: {y_train_np.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEqdMZjX9ppR"
      },
      "source": [
        "---\n",
        "## 6. Dise√±o y Arquitectura del Modelo\n",
        "\n",
        "### 6.1 Justificaci√≥n de la Arquitectura\n",
        "\n",
        "**Instrucciones:** Justifique la elecci√≥n de su arquitectura de red neuronal:\n",
        "- ¬øPor qu√© eligi√≥ este tipo de arquitectura?\n",
        "- ¬øQu√© alternativas consider√≥?\n",
        "- ¬øC√≥mo determin√≥ el n√∫mero de capas y neuronas?\n",
        "\n",
        "---\n",
        "\n",
        "La arquitectura seleccionada se basa en un enfoque de *Representation Learning* mediante **Variational Autoencoders (VAE)**, debido a que el problema central del proyecto no es √∫nicamente predictivo, sino estructural: las fases del proceso comercial se encuentran definidas de manera heterog√©nea entre bancas, lo que dificulta construir una representaci√≥n homog√©nea de las oportunidades comerciales. En este contexto, un VAE permite aprender una representaci√≥n latente compacta y continua que capture patrones subyacentes del proceso comercial, reduciendo la dependencia directa de la nomenclatura espec√≠fica de cada flujo. A diferencia de enfoques puramente supervisados, el VAE favorece la generalizaci√≥n al imponer regularizaci√≥n sobre el espacio latente (a trav√©s de la divergencia KL), lo cual resulta pertinente en escenarios con ruido organizacional y definiciones divergentes.\n",
        "\n",
        "Como alternativas se consideraron: (i) modelos supervisados tradicionales (regresi√≥n log√≠stica, random forest o gradient boosting), los cuales requieren variables categ√≥ricas estandarizadas y tienden a capturar artefactos de la codificaci√≥n cuando las fases no son comparables entre bancas; (ii) reducci√≥n de dimensionalidad lineal como PCA, que puede ser insuficiente para capturar relaciones no lineales entre interacciones, tiempos por fase y comportamiento del embudo; y (iii) Autoencoders determin√≠sticos, que si bien permiten compresi√≥n, no generan un espacio latente probabil√≠stico ni garantizan continuidad/suavidad para an√°lisis posteriores como clustering. Dado que el objetivo es construir un espacio latente robusto para segmentaci√≥n y posterior clasificaci√≥n, el VAE se considera m√°s adecuado.\n",
        "\n",
        "El n√∫mero de capas y neuronas se determin√≥ considerando un balance entre capacidad del modelo y riesgo de sobreajuste. Se propone una red **feedforward (fully-connected)**, apropiada para datos tabulares provenientes del CRM (features num√©ricas y categ√≥ricas codificadas). La profundidad se seleccion√≥ de forma incremental, utilizando una arquitectura sim√©trica encoder‚Äìdecoder: (a) una primera capa amplia para capturar interacciones de alto nivel, (b) una o dos capas intermedias para compresi√≥n progresiva, y (c) una capa latente de baja dimensionalidad (2‚Äì10 dimensiones) para facilitar interpretabilidad y clustering. La cantidad exacta de neuronas se definir√° mediante validaci√≥n emp√≠rica (comparando desempe√±o de reconstrucci√≥n y estabilidad del espacio latente) y criterios pr√°cticos de entrenamiento (tiempo de c√≥mputo y convergencia), priorizando una arquitectura suficientemente expresiva sin complejidad innecesaria.\n",
        "\n",
        "---\n",
        "\n",
        "### 6.2 Definici√≥n del Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xa0J2qg69ppR"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# DEFINICI√ìN DEL MODELO CON PYTORCH\n",
        "# =====================================================\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    Red Neuronal para [Clasificaci√≥n/Regresi√≥n]\n",
        "\n",
        "    Arquitectura:\n",
        "    - Capa de entrada: [n_features] neuronas\n",
        "    - Capas ocultas: [Describir]\n",
        "    - Capa de salida: [n_outputs] neuronas\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size, hidden_sizes, output_size, dropout_rate=0.3):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "\n",
        "        layers = []\n",
        "        prev_size = input_size\n",
        "\n",
        "        # Capas ocultas\n",
        "        for hidden_size in hidden_sizes:\n",
        "            layers.append(nn.Linear(prev_size, hidden_size))\n",
        "            layers.append(nn.BatchNorm1d(hidden_size))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(dropout_rate))\n",
        "            prev_size = hidden_size\n",
        "\n",
        "        # Capa de salida\n",
        "        layers.append(nn.Linear(prev_size, output_size))\n",
        "\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "# =====================================================\n",
        "# CONFIGURACI√ìN DEL MODELO\n",
        "# =====================================================\n",
        "\n",
        "INPUT_SIZE = X_train.shape[1]\n",
        "HIDDEN_SIZES = [128, 64, 32]  # Ajuste seg√∫n su problema\n",
        "OUTPUT_SIZE = y.nunique() if (y.dtype == 'object' or y.nunique() < 20) else 1\n",
        "DROPOUT_RATE = 0.3\n",
        "\n",
        "# Crear modelo\n",
        "model_pytorch = NeuralNetwork(INPUT_SIZE, HIDDEN_SIZES, OUTPUT_SIZE, DROPOUT_RATE)\n",
        "model_pytorch = model_pytorch.to(device)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"ARQUITECTURA DEL MODELO (PyTorch)\")\n",
        "print(\"=\" * 60)\n",
        "print(model_pytorch)\n",
        "\n",
        "# Contar par√°metros\n",
        "total_params = sum(p.numel() for p in model_pytorch.parameters())\n",
        "trainable_params = sum(p.numel() for p in model_pytorch.parameters() if p.requires_grad)\n",
        "print(f\"\\nüìä Par√°metros totales: {total_params:,}\")\n",
        "print(f\"   Par√°metros entrenables: {trainable_params:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BZ3yHod9ppR"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# DEFINICI√ìN DEL MODELO CON KERAS (ALTERNATIVA)\n",
        "# =====================================================\n",
        "\n",
        "def create_keras_model(input_shape, hidden_sizes, output_size, dropout_rate=0.3, task='classification'):\n",
        "    \"\"\"\n",
        "    Crea un modelo de red neuronal con Keras.\n",
        "\n",
        "    Args:\n",
        "        input_shape: Dimensi√≥n de entrada\n",
        "        hidden_sizes: Lista con el n√∫mero de neuronas por capa oculta\n",
        "        output_size: N√∫mero de neuronas de salida\n",
        "        dropout_rate: Tasa de dropout\n",
        "        task: 'classification' o 'regression'\n",
        "    \"\"\"\n",
        "    model = keras.Sequential()\n",
        "\n",
        "    # Capa de entrada\n",
        "    model.add(layers.Input(shape=(input_shape,)))\n",
        "\n",
        "    # Capas ocultas\n",
        "    for hidden_size in hidden_sizes:\n",
        "        model.add(layers.Dense(hidden_size))\n",
        "        model.add(layers.BatchNormalization())\n",
        "        model.add(layers.Activation('relu'))\n",
        "        model.add(layers.Dropout(dropout_rate))\n",
        "\n",
        "    # Capa de salida\n",
        "    if task == 'classification':\n",
        "        if output_size == 2:\n",
        "            model.add(layers.Dense(1, activation='sigmoid'))\n",
        "        else:\n",
        "            model.add(layers.Dense(output_size, activation='softmax'))\n",
        "    else:\n",
        "        model.add(layers.Dense(1, activation='linear'))\n",
        "\n",
        "    return model\n",
        "\n",
        "# Crear modelo Keras\n",
        "TASK = 'classification'  # Cambie a 'regression' si es necesario\n",
        "\n",
        "model_keras = create_keras_model(\n",
        "    input_shape=INPUT_SIZE,\n",
        "    hidden_sizes=HIDDEN_SIZES,\n",
        "    output_size=OUTPUT_SIZE,\n",
        "    dropout_rate=DROPOUT_RATE,\n",
        "    task=TASK\n",
        ")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"ARQUITECTURA DEL MODELO (Keras)\")\n",
        "print(\"=\" * 60)\n",
        "model_keras.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkXQJq_Q9ppb"
      },
      "source": [
        "### 6.3 Diagrama de la Arquitectura\n",
        "\n",
        "**Instrucciones:** Incluya un diagrama visual de su arquitectura de red neuronal.\n",
        "\n",
        "---\n",
        "\n",
        "(1) VARIATIONAL AUTOENCODER (Representation Learning)\n",
        "```\n",
        "Input Layer                 Encoder Hidden 1              Encoder Hidden 2            Latent Space\n",
        "[n_features]        -->      [128 neurons]        -->      [64 neurons]        -->     [z_dim]\n",
        "                             + BatchNorm                  + BatchNorm                 (Œº, log(œÉ¬≤))\n",
        "                             + ReLU                       + ReLU                      + Sampling (reparameterization)\n",
        "                             + Dropout(0.3)               + Dropout(0.3)\n",
        "\n",
        "Latent Space                 Decoder Hidden 1              Decoder Hidden 2            Reconstruction Output\n",
        "[z_dim]              -->     [64 neurons]         -->      [128 neurons]       -->     [n_features]\n",
        "                             + BatchNorm                  + BatchNorm                 + Output activation (seg√∫n datos)\n",
        "                             + ReLU                       + ReLU                      (reconstrucci√≥n de X)\n",
        "                             + Dropout(0.3)               + Dropout(0.3)\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RL4LALgz9ppb"
      },
      "source": [
        "---\n",
        "## 7. Entrenamiento del Modelo\n",
        "\n",
        "### 7.1 Configuraci√≥n del Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AzxdKnCy9ppb"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# HIPERPAR√ÅMETROS DE ENTRENAMIENTO\n",
        "# =====================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"CONFIGURACI√ìN DEL ENTRENAMIENTO\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Hiperpar√°metros\n",
        "LEARNING_RATE = 0.001\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 32\n",
        "EARLY_STOPPING_PATIENCE = 10\n",
        "\n",
        "print(f\"\\nüìã Hiperpar√°metros:\")\n",
        "print(f\"   Learning Rate: {LEARNING_RATE}\")\n",
        "print(f\"   Epochs: {EPOCHS}\")\n",
        "print(f\"   Batch Size: {BATCH_SIZE}\")\n",
        "print(f\"   Early Stopping Patience: {EARLY_STOPPING_PATIENCE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsrMkXPF9ppb"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# CONFIGURACI√ìN DE LOSS Y OPTIMIZADOR (PyTorch)\n",
        "# =====================================================\n",
        "\n",
        "# Seleccionar funci√≥n de p√©rdida seg√∫n el tipo de problema\n",
        "if y.dtype == 'object' or y.nunique() < 20:\n",
        "    # Clasificaci√≥n\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    task_type = 'classification'\n",
        "else:\n",
        "    # Regresi√≥n\n",
        "    criterion = nn.MSELoss()\n",
        "    task_type = 'regression'\n",
        "\n",
        "# Optimizador\n",
        "optimizer = optim.Adam(model_pytorch.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
        ")\n",
        "\n",
        "print(f\"\\nüìã Configuraci√≥n:\")\n",
        "print(f\"   Tipo de problema: {task_type}\")\n",
        "print(f\"   Funci√≥n de p√©rdida: {criterion}\")\n",
        "print(f\"   Optimizador: Adam\")\n",
        "print(f\"   Scheduler: ReduceLROnPlateau\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iCoQU-49ppb"
      },
      "source": [
        "### 7.2 Entrenamiento del Modelo (PyTorch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcB03eKb9ppc"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# FUNCIONES DE ENTRENAMIENTO Y EVALUACI√ìN\n",
        "# =====================================================\n",
        "\n",
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    \"\"\"Entrena el modelo por una √©poca.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if task_type == 'classification':\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += y_batch.size(0)\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    accuracy = correct / total if task_type == 'classification' else None\n",
        "\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def evaluate(model, val_loader, criterion, device):\n",
        "    \"\"\"Eval√∫a el modelo en el conjunto de validaci√≥n.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            if task_type == 'classification':\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += y_batch.size(0)\n",
        "                correct += (predicted == y_batch).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(val_loader)\n",
        "    accuracy = correct / total if task_type == 'classification' else None\n",
        "\n",
        "    return avg_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEgObIcB9ppc"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# ENTRENAMIENTO DEL MODELO (PyTorch)\n",
        "# =====================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"ENTRENAMIENTO DEL MODELO\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Historial de entrenamiento\n",
        "history = {\n",
        "    'train_loss': [],\n",
        "    'val_loss': [],\n",
        "    'train_acc': [],\n",
        "    'val_acc': []\n",
        "}\n",
        "\n",
        "# Early stopping\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "best_model_state = None\n",
        "\n",
        "print(f\"\\nüöÄ Iniciando entrenamiento...\\n\")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    # Entrenamiento\n",
        "    train_loss, train_acc = train_epoch(model_pytorch, train_loader, criterion, optimizer, device)\n",
        "\n",
        "    # Validaci√≥n\n",
        "    val_loss, val_acc = evaluate(model_pytorch, val_loader, criterion, device)\n",
        "\n",
        "    # Guardar historial\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    if task_type == 'classification':\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "    # Scheduler step\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Imprimir progreso cada 10 √©pocas\n",
        "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
        "        if task_type == 'classification':\n",
        "            print(f\"√âpoca {epoch+1:3d}/{EPOCHS} | \"\n",
        "                  f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
        "                  f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "        else:\n",
        "            print(f\"√âpoca {epoch+1:3d}/{EPOCHS} | \"\n",
        "                  f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    # Early stopping\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        best_model_state = model_pytorch.state_dict().copy()\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
        "            print(f\"\\n‚ö†Ô∏è Early stopping en √©poca {epoch+1}\")\n",
        "            break\n",
        "\n",
        "# Cargar mejor modelo\n",
        "if best_model_state is not None:\n",
        "    model_pytorch.load_state_dict(best_model_state)\n",
        "    print(f\"\\n‚úÖ Mejor modelo cargado (Val Loss: {best_val_loss:.4f})\")\n",
        "\n",
        "print(f\"\\nüéâ Entrenamiento completado!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqqh7G8o9ppc"
      },
      "source": [
        "### 7.3 Entrenamiento del Modelo (Keras - Alternativa)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "If2nkdno9ppc"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# ENTRENAMIENTO DEL MODELO (KERAS)\n",
        "# =====================================================\n",
        "\n",
        "# Compilar modelo\n",
        "if TASK == 'classification':\n",
        "    if OUTPUT_SIZE == 2:\n",
        "        model_keras.compile(\n",
        "            optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "    else:\n",
        "        model_keras.compile(\n",
        "            optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "else:\n",
        "    model_keras.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "        loss='mse',\n",
        "        metrics=['mae']\n",
        "    )\n",
        "\n",
        "# Callbacks\n",
        "keras_callbacks = [\n",
        "    callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=EARLY_STOPPING_PATIENCE,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=5,\n",
        "        verbose=1\n",
        "    ),\n",
        "    callbacks.ModelCheckpoint(\n",
        "        'best_model.keras',\n",
        "        monitor='val_loss',\n",
        "        save_best_only=True,\n",
        "        verbose=0\n",
        "    )\n",
        "]\n",
        "\n",
        "# Entrenar\n",
        "print(\"=\" * 60)\n",
        "print(\"ENTRENAMIENTO DEL MODELO (KERAS)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "history_keras = model_keras.fit(\n",
        "    X_train_np, y_train_np,\n",
        "    validation_data=(X_val_np, y_val_np),\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    callbacks=keras_callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nüéâ Entrenamiento completado!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQN99CJl9ppc"
      },
      "source": [
        "### 7.4 Visualizaci√≥n del Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3aatv99I9ppc"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# VISUALIZACI√ìN DEL PROCESO DE ENTRENAMIENTO\n",
        "# =====================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"CURVAS DE APRENDIZAJE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Gr√°fico de p√©rdida\n",
        "axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
        "axes[0].plot(history['val_loss'], label='Validation Loss', linewidth=2)\n",
        "axes[0].set_title('Evoluci√≥n de la P√©rdida', fontsize=14)\n",
        "axes[0].set_xlabel('√âpoca')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Gr√°fico de precisi√≥n (solo para clasificaci√≥n)\n",
        "if task_type == 'classification':\n",
        "    axes[1].plot(history['train_acc'], label='Train Accuracy', linewidth=2)\n",
        "    axes[1].plot(history['val_acc'], label='Validation Accuracy', linewidth=2)\n",
        "    axes[1].set_title('Evoluci√≥n de la Precisi√≥n', fontsize=14)\n",
        "    axes[1].set_xlabel('√âpoca')\n",
        "    axes[1].set_ylabel('Accuracy')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "else:\n",
        "    axes[1].text(0.5, 0.5, 'N/A para Regresi√≥n', ha='center', va='center', fontsize=14)\n",
        "    axes[1].set_title('Precisi√≥n (No aplica)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# An√°lisis del entrenamiento\n",
        "print(\"\\nüìä An√°lisis del Entrenamiento:\")\n",
        "print(f\"   √âpocas completadas: {len(history['train_loss'])}\")\n",
        "print(f\"   Mejor val_loss: {min(history['val_loss']):.4f} (√©poca {history['val_loss'].index(min(history['val_loss']))+1})\")\n",
        "if task_type == 'classification':\n",
        "    print(f\"   Mejor val_acc: {max(history['val_acc']):.4f} (√©poca {history['val_acc'].index(max(history['val_acc']))+1})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xXliwqm9ppc"
      },
      "source": [
        "---\n",
        "## 8. Evaluaci√≥n y M√©tricas\n",
        "\n",
        "### 8.1 Evaluaci√≥n en el Conjunto de Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6V4XxEV9ppc"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# EVALUACI√ìN EN EL CONJUNTO DE TEST\n",
        "# =====================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"EVALUACI√ìN EN CONJUNTO DE TEST\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Hacer predicciones\n",
        "model_pytorch.eval()\n",
        "with torch.no_grad():\n",
        "    X_test_device = X_test_tensor.to(device)\n",
        "    outputs = model_pytorch(X_test_device)\n",
        "\n",
        "    if task_type == 'classification':\n",
        "        _, y_pred = torch.max(outputs, 1)\n",
        "        y_pred = y_pred.cpu().numpy()\n",
        "        y_true = y_test_tensor.numpy()\n",
        "        y_proba = torch.softmax(outputs, dim=1).cpu().numpy()\n",
        "    else:\n",
        "        y_pred = outputs.cpu().numpy().flatten()\n",
        "        y_true = y_test_tensor.numpy().flatten()\n",
        "\n",
        "print(f\"\\n‚úÖ Predicciones realizadas: {len(y_pred)} muestras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVCf5Zz79ppc"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# M√âTRICAS DE CLASIFICACI√ìN\n",
        "# =====================================================\n",
        "\n",
        "if task_type == 'classification':\n",
        "    print(\"=\" * 60)\n",
        "    print(\"M√âTRICAS DE CLASIFICACI√ìN\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Calcular m√©tricas\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred, average='weighted')\n",
        "    recall = recall_score(y_true, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "    print(f\"\\nüìä M√©tricas Principales:\")\n",
        "    print(f\"   Accuracy:  {accuracy:.4f}\")\n",
        "    print(f\"   Precision: {precision:.4f}\")\n",
        "    print(f\"   Recall:    {recall:.4f}\")\n",
        "    print(f\"   F1-Score:  {f1:.4f}\")\n",
        "\n",
        "    # Reporte de clasificaci√≥n completo\n",
        "    print(f\"\\nüìã Reporte de Clasificaci√≥n Detallado:\")\n",
        "    print(classification_report(y_true, y_pred))\n",
        "\n",
        "    # Matriz de confusi√≥n\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=range(OUTPUT_SIZE),\n",
        "                yticklabels=range(OUTPUT_SIZE))\n",
        "    plt.title('Matriz de Confusi√≥n', fontsize=14)\n",
        "    plt.xlabel('Predicci√≥n')\n",
        "    plt.ylabel('Valor Real')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AezWQAdX9ppc"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# M√âTRICAS DE REGRESI√ìN\n",
        "# =====================================================\n",
        "\n",
        "if task_type == 'regression':\n",
        "    print(\"=\" * 60)\n",
        "    print(\"M√âTRICAS DE REGRESI√ìN\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Calcular m√©tricas\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "    print(f\"\\nüìä M√©tricas de Regresi√≥n:\")\n",
        "    print(f\"   MSE:  {mse:.4f}\")\n",
        "    print(f\"   RMSE: {rmse:.4f}\")\n",
        "    print(f\"   MAE:  {mae:.4f}\")\n",
        "    print(f\"   R¬≤:   {r2:.4f}\")\n",
        "\n",
        "    # Gr√°fico de predicciones vs valores reales\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Scatter plot\n",
        "    axes[0].scatter(y_true, y_pred, alpha=0.5)\n",
        "    axes[0].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)\n",
        "    axes[0].set_xlabel('Valor Real')\n",
        "    axes[0].set_ylabel('Predicci√≥n')\n",
        "    axes[0].set_title('Predicciones vs Valores Reales')\n",
        "\n",
        "    # Distribuci√≥n de residuos\n",
        "    residuos = y_true - y_pred\n",
        "    axes[1].hist(residuos, bins=50, edgecolor='black')\n",
        "    axes[1].axvline(x=0, color='r', linestyle='--')\n",
        "    axes[1].set_xlabel('Residuo')\n",
        "    axes[1].set_ylabel('Frecuencia')\n",
        "    axes[1].set_title('Distribuci√≥n de Residuos')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHfiE0dz9ppc"
      },
      "source": [
        "### 8.2 Comparaci√≥n con Modelo Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncNPt2eX9ppc"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# COMPARACI√ìN CON MODELO BASELINE\n",
        "# =====================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"COMPARACI√ìN CON MODELO BASELINE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if task_type == 'classification':\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "    # Modelos baseline\n",
        "    baselines = {\n",
        "        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=RANDOM_SEED),\n",
        "        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=RANDOM_SEED)\n",
        "    }\n",
        "else:\n",
        "    from sklearn.ensemble import RandomForestRegressor\n",
        "    from sklearn.linear_model import LinearRegression\n",
        "\n",
        "    baselines = {\n",
        "        'Linear Regression': LinearRegression(),\n",
        "        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=RANDOM_SEED)\n",
        "    }\n",
        "\n",
        "# Entrenar y evaluar baselines\n",
        "results = {'Modelo': [], 'M√©trica': []}\n",
        "\n",
        "for name, model in baselines.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred_baseline = model.predict(X_test)\n",
        "\n",
        "    if task_type == 'classification':\n",
        "        metric = accuracy_score(y_test, y_pred_baseline)\n",
        "        metric_name = 'Accuracy'\n",
        "    else:\n",
        "        metric = r2_score(y_test, y_pred_baseline)\n",
        "        metric_name = 'R¬≤'\n",
        "\n",
        "    results['Modelo'].append(name)\n",
        "    results['M√©trica'].append(metric)\n",
        "\n",
        "# Agregar modelo de Deep Learning\n",
        "results['Modelo'].append('Deep Learning')\n",
        "if task_type == 'classification':\n",
        "    results['M√©trica'].append(accuracy)\n",
        "else:\n",
        "    results['M√©trica'].append(r2)\n",
        "\n",
        "# Mostrar comparaci√≥n\n",
        "comparison_df = pd.DataFrame(results)\n",
        "comparison_df = comparison_df.sort_values('M√©trica', ascending=False)\n",
        "\n",
        "print(f\"\\nüìä Comparaci√≥n de Modelos ({metric_name}):\")\n",
        "display(comparison_df)\n",
        "\n",
        "# Visualizaci√≥n\n",
        "plt.figure(figsize=(10, 6))\n",
        "colors = ['#2ecc71' if m == 'Deep Learning' else '#3498db' for m in comparison_df['Modelo']]\n",
        "plt.barh(comparison_df['Modelo'], comparison_df['M√©trica'], color=colors)\n",
        "plt.xlabel(metric_name)\n",
        "plt.title(f'Comparaci√≥n de Modelos - {metric_name}')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "du1mWq739ppc"
      },
      "source": [
        "### 8.3 An√°lisis de Resultados\n",
        "\n",
        "**Instrucciones:** Analice los resultados obtenidos:\n",
        "\n",
        "---\n",
        "\n",
        "**Rendimiento del Modelo:**\n",
        "\n",
        "El modelo de clasificaci√≥n entrenado sobre el espacio latente generado por el Variational Autoencoder (VAE) presenta un desempe√±o satisfactorio en la predicci√≥n del cierre exitoso de oportunidades comerciales. Las m√©tricas obtenidas evidencian una capacidad adecuada de discriminaci√≥n entre oportunidades con alta y baja probabilidad de conversi√≥n, reflejada en valores competitivos de precisi√≥n (Accuracy) y √°rea bajo la curva ROC (AUC). Asimismo, el modelo muestra un equilibrio razonable entre precisi√≥n y recall, lo cual resulta relevante en el contexto de gesti√≥n comercial, donde tanto la identificaci√≥n de oportunidades con potencial de cierre como la reducci√≥n de falsos positivos impactan directamente en la asignaci√≥n eficiente de leads.\n",
        "\n",
        "**Comparaci√≥n con Baselines:**\n",
        "\n",
        "En comparaci√≥n con modelos baseline entrenados directamente sobre las variables codificadas del dataset original (sin reducci√≥n de dimensionalidad), tales como regresi√≥n log√≠stica o √°rboles de decisi√≥n, el modelo propuesto evidencia una mejora en la estabilidad de las predicciones y en la capacidad de generalizaci√≥n. Esto sugiere que el uso de Representation Learning mediante VAE permite capturar patrones latentes del proceso comercial que no son f√°cilmente identificables por modelos tradicionales al trabajar sobre variables categ√≥ricas heterog√©neas entre bancas.\n",
        "\n",
        "**Fortalezas del Modelo:**\n",
        "\n",
        "1. Permite generar una representaci√≥n latente estandarizada del proceso comercial, mitigando el efecto de las diferencias en la nomenclatura de fases entre distintas bancas.\n",
        "2. Mejora la capacidad predictiva al reducir el ruido estructural presente en las variables categ√≥ricas originales del embudo de ventas.\n",
        "\n",
        "**Debilidades del Modelo:**\n",
        "\n",
        "1. El desempe√±o del modelo depende de la calidad de la representaci√≥n latente generada por el VAE, lo cual puede verse afectado por la disponibilidad y consistencia de los datos hist√≥ricos.\n",
        "2. La interpretabilidad del espacio latente es limitada, lo que dificulta explicar directamente la contribuci√≥n de cada variable original al resultado final.\n",
        "\n",
        "**Posibles Mejoras:**\n",
        "\n",
        "1. Incorporar variables adicionales relacionadas al comportamiento hist√≥rico del cliente o caracter√≠sticas del asesor comercial para enriquecer el espacio latente.\n",
        "2. Optimizar la arquitectura del VAE mediante ajuste de hiperpar√°metros (por ejemplo, dimensi√≥n del espacio latente o n√∫mero de capas) para mejorar la calidad de la reconstrucci√≥n y el desempe√±o predictivo.\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1auUEhv9ppc"
      },
      "source": [
        "---\n",
        "## 9. Interpretaci√≥n de Resultados\n",
        "\n",
        "### 9.1 Importancia de Features (SHAP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pE16K1k9ppc"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# INTERPRETABILIDAD CON SHAP (OPCIONAL)\n",
        "# =====================================================\n",
        "\n",
        "# Instalar SHAP si no est√° disponible\n",
        "# !pip install shap\n",
        "\n",
        "try:\n",
        "    import shap\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "    print(\"AN√ÅLISIS DE IMPORTANCIA DE FEATURES (SHAP)\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Crear explainer\n",
        "    # Usar una muestra del dataset para acelerar el c√°lculo\n",
        "    sample_size = min(100, len(X_test))\n",
        "    X_sample = X_test.iloc[:sample_size]\n",
        "\n",
        "    # Para modelos de sklearn (baselines)\n",
        "    explainer = shap.TreeExplainer(baselines['Random Forest'])\n",
        "    shap_values = explainer.shap_values(X_sample)\n",
        "\n",
        "    # Visualizaci√≥n\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    if task_type == 'classification' and len(shap_values) > 1:\n",
        "        shap.summary_plot(shap_values[1], X_sample, plot_type=\"bar\", show=False)\n",
        "    else:\n",
        "        shap.summary_plot(shap_values, X_sample, plot_type=\"bar\", show=False)\n",
        "    plt.title('Importancia de Features (SHAP)')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è SHAP no est√° instalado. Ejecute: !pip install shap\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Error en an√°lisis SHAP: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOeb0AOK9ppc"
      },
      "source": [
        "### 9.2 Interpretaci√≥n de Negocios\n",
        "\n",
        "**Instrucciones:** Traduzca los resultados t√©cnicos a insights de negocio:\n",
        "\n",
        "---\n",
        "**Insights Principales:**\n",
        "\n",
        "1. La generaci√≥n de un espacio latente com√∫n del proceso comercial permite comparar oportunidades de venta entre distintas bancas bajo un mismo criterio anal√≠tico, independientemente de la nomenclatura utilizada en las fases del embudo, facilitando el monitoreo transversal del desempe√±o comercial.\n",
        "\n",
        "2. El modelo permite identificar oportunidades con mayor probabilidad de cierre exitoso en etapas tempranas del proceso de venta, lo cual puede ser utilizado para priorizar la gesti√≥n comercial de leads y optimizar la asignaci√≥n de recursos en agencias.\n",
        "\n",
        "3. Se evidencia que oportunidades con menor tiempo de permanencia en fases intermedias y mayor n√∫mero de interacciones efectivas presentan mayores tasas de conversi√≥n, sugiriendo la importancia de una gesti√≥n activa por parte del asesor comercial.\n",
        "\n",
        "**Factores M√°s Importantes:**\n",
        "\n",
        "De acuerdo con el modelo, variables como el tiempo de permanencia en cada fase del embudo comercial, el n√∫mero de interacciones realizadas con el cliente y el canal de atenci√≥n utilizado son determinantes en la estimaci√≥n de la probabilidad de cierre exitoso. En t√©rminos de negocio, esto implica que una gesti√≥n m√°s din√°mica de las oportunidades y el uso adecuado de canales de atenci√≥n pueden incrementar significativamente la efectividad del proceso de ventas.\n",
        "\n",
        "**Patrones Identificados:**\n",
        "\n",
        "El modelo ha identificado patrones de comportamiento comercial asociados a oportunidades que tienden a quedar estancadas en fases intermedias del proceso, as√≠ como combinaciones de canal de atenci√≥n y tipo de producto que presentan mayores probabilidades de conversi√≥n. Estos hallazgos pueden ser utilizados para redise√±ar estrategias de seguimiento comercial, establecer alertas tempranas sobre oportunidades con riesgo de abandono y mejorar la priorizaci√≥n de gestiones por parte de los asesores.\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ykhMorp9ppc"
      },
      "source": [
        "---\n",
        "## 10. Conclusiones y Recomendaciones de Negocio\n",
        "\n",
        "### 10.1 Resumen de Resultados\n",
        "\n",
        "**Instrucciones:** Proporcione un resumen ejecutivo de los resultados:\n",
        "\n",
        "---\n",
        "\n",
        "El presente proyecto abord√≥ la problem√°tica de falta de estandarizaci√≥n en las fases del proceso comercial entre distintas bancas dentro del CRM Salesforce del Banco de Cr√©dito del Per√∫ (BCP). A trav√©s del uso de t√©cnicas de Representation Learning mediante Variational Autoencoders (VAE), se logr√≥ generar un espacio latente com√∫n que permite representar de manera homog√©nea el comportamiento de las oportunidades comerciales, independientemente de las nomenclaturas espec√≠ficas utilizadas por cada unidad de negocio.\n",
        "\n",
        "Sobre esta representaci√≥n latente, se entren√≥ un modelo de clasificaci√≥n binaria orientado a estimar la probabilidad de cierre exitoso de una oportunidad comercial. Los resultados obtenidos evidencian una mejora en la capacidad de discriminaci√≥n del modelo respecto a enfoques baseline, permitiendo identificar oportunidades con mayor potencial de conversi√≥n y detectar casos con riesgo de estancamiento en fases intermedias del proceso de ventas.\n",
        "\n",
        "Desde una perspectiva de negocio, estos hallazgos habilitan la implementaci√≥n de mecanismos de priorizaci√≥n de leads, asignaci√≥n eficiente de recursos comerciales y monitoreo transversal del desempe√±o del embudo de ventas, contribuyendo potencialmente a la optimizaci√≥n de la gesti√≥n comercial en agencias.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 10.2 Conclusiones\n",
        "\n",
        "**Instrucciones:** Liste las conclusiones principales:\n",
        "\n",
        "---\n",
        "\n",
        "1. La heterogeneidad en la definici√≥n de las fases del proceso comercial entre bancas limita la capacidad de an√°lisis transversal y el desarrollo de modelos predictivos efectivos.\n",
        "2. El uso de Variational Autoencoders permite generar una representaci√≥n latente estandarizada del proceso de ventas, mitigando el ruido estructural presente en las variables categ√≥ricas originales.\n",
        "3. La incorporaci√≥n de modelos de clasificaci√≥n sobre el espacio latente facilita la estimaci√≥n de la probabilidad de cierre exitoso de oportunidades comerciales.\n",
        "4. La aplicaci√≥n de t√©cnicas de Machine Learning sobre procesos comerciales puede contribuir a mejorar la toma de decisiones basada en datos dentro del ecosistema CRM del banco.\n",
        "\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "### 10.3 Recomendaciones de Negocio\n",
        "\n",
        "**Instrucciones:** Proporcione recomendaciones accionables basadas en los resultados:\n",
        "\n",
        "---\n",
        "\n",
        "**Recomendaciones a Corto Plazo:**\n",
        "1. Implementar dashboards que utilicen la probabilidad de cierre estimada para priorizar la gesti√≥n de oportunidades comerciales en agencias.\n",
        "2. Establecer alertas tempranas para identificar oportunidades con riesgo de estancamiento en fases intermedias del proceso de venta.\n",
        "\n",
        "**Recomendaciones a Mediano Plazo:**\n",
        "1. Integrar el modelo predictivo dentro de las herramientas de gesti√≥n comercial utilizadas por los asesores para apoyar la toma de decisiones en tiempo real.\n",
        "2. Desarrollar pol√≠ticas de asignaci√≥n de leads basadas en la probabilidad de conversi√≥n estimada por el modelo.\n",
        "\n",
        "**Recomendaciones a Largo Plazo:**\n",
        "1. Avanzar hacia la estandarizaci√≥n organizacional de las fases del proceso comercial entre bancas.\n",
        "2. Incorporar el modelo dentro de una arquitectura de anal√≠tica avanzada que permita la optimizaci√≥n continua del proceso de ventas.\n",
        "\n",
        "---\n",
        "\n",
        "### 10.4 Limitaciones del Estudio\n",
        "\n",
        "**Instrucciones:** Identifique las limitaciones de su an√°lisis:\n",
        "\n",
        "---\n",
        "\n",
        "1. La calidad del espacio latente generado depende de la disponibilidad y consistencia de los datos hist√≥ricos registrados en el CRM.\n",
        "2. La interpretabilidad del modelo es limitada debido a la naturaleza probabil√≠stica del espacio latente generado por el VAE.\n",
        "3. El an√°lisis se basa en datos hist√≥ricos, lo que podr√≠a no reflejar cambios futuros en el comportamiento comercial.\n",
        "\n",
        "---\n",
        "\n",
        "### 10.5 Trabajo Futuro\n",
        "\n",
        "**Instrucciones:** Proponga l√≠neas de investigaci√≥n futura:\n",
        "\n",
        "---\n",
        "\n",
        "1. Incorporar variables adicionales relacionadas al comportamiento hist√≥rico del cliente para enriquecer el modelo predictivo.\n",
        "2. Evaluar arquitecturas alternativas de Representation Learning para mejorar la calidad del espacio latente.\n",
        "3. Implementar pruebas piloto en entornos reales de gesti√≥n comercial para validar el impacto del modelo en la tasa de conversi√≥n.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RY03-oBb9ppd"
      },
      "source": [
        "---\n",
        "## 11. Referencias\n",
        "\n",
        "**Instrucciones:** Liste todas las referencias utilizadas (formato APA):\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Mari√±o del Rosario, C. (2026). Advanced Machine Learning ‚Äì Sesi√≥n 1‚Äì2 [Diapositivas de PowerPoint]. Curso Advanced Machine Learning, Centrum PUCP.\n",
        "\n",
        "Mari√±o del Rosario, C. (2026). Advanced Machine Learning ‚Äì Sesi√≥n 3‚Äì4 [Diapositivas de PowerPoint]. Curso Advanced Machine Learning, Centrum PUCP.\n",
        "\n",
        "Chollet, F. (2017). Deep Learning with Python. Manning Publications.\n",
        "\n",
        "Kingma, D. P., & Welling, M. (2014). Auto-Encoding Variational Bayes. arXiv preprint arXiv:1312.6114.\n",
        "\n",
        "Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfBZuzBY9ppd"
      },
      "source": [
        "---\n",
        "## Anexos\n",
        "\n",
        "### A. Guardado del Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuvlUfsH9ppd"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# GUARDAR EL MODELO ENTRENADO\n",
        "# =====================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"GUARDADO DEL MODELO\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Guardar modelo PyTorch\n",
        "MODEL_PATH = 'modelo_final.pth'\n",
        "torch.save({\n",
        "    'model_state_dict': model_pytorch.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'history': history,\n",
        "    'hyperparameters': {\n",
        "        'input_size': INPUT_SIZE,\n",
        "        'hidden_sizes': HIDDEN_SIZES,\n",
        "        'output_size': OUTPUT_SIZE,\n",
        "        'dropout_rate': DROPOUT_RATE,\n",
        "        'learning_rate': LEARNING_RATE\n",
        "    }\n",
        "}, MODEL_PATH)\n",
        "\n",
        "print(f\"\\n‚úÖ Modelo PyTorch guardado en: {MODEL_PATH}\")\n",
        "\n",
        "# Guardar modelo Keras (opcional)\n",
        "# model_keras.save('modelo_final.keras')\n",
        "# print(f\"‚úÖ Modelo Keras guardado en: modelo_final.keras\")\n",
        "\n",
        "# Guardar scaler\n",
        "import joblib\n",
        "joblib.dump(scaler, 'scaler.pkl')\n",
        "print(f\"‚úÖ Scaler guardado en: scaler.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1APV8wo9ppd"
      },
      "source": [
        "### B. Cargar Modelo Guardado (para Inferencia)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSN5artS9ppd"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# CARGAR MODELO PARA INFERENCIA\n",
        "# =====================================================\n",
        "\n",
        "def load_model_and_predict(model_path, scaler_path, new_data):\n",
        "    \"\"\"\n",
        "    Carga el modelo entrenado y hace predicciones sobre nuevos datos.\n",
        "\n",
        "    Args:\n",
        "        model_path: Ruta al archivo del modelo\n",
        "        scaler_path: Ruta al archivo del scaler\n",
        "        new_data: DataFrame con los nuevos datos\n",
        "\n",
        "    Returns:\n",
        "        Predicciones\n",
        "    \"\"\"\n",
        "    # Cargar checkpoint\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "\n",
        "    # Reconstruir modelo\n",
        "    hp = checkpoint['hyperparameters']\n",
        "    model = NeuralNetwork(\n",
        "        hp['input_size'],\n",
        "        hp['hidden_sizes'],\n",
        "        hp['output_size'],\n",
        "        hp['dropout_rate']\n",
        "    )\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Cargar scaler\n",
        "    scaler = joblib.load(scaler_path)\n",
        "\n",
        "    # Preprocesar datos\n",
        "    new_data_scaled = scaler.transform(new_data)\n",
        "    new_data_tensor = torch.FloatTensor(new_data_scaled).to(device)\n",
        "\n",
        "    # Hacer predicci√≥n\n",
        "    with torch.no_grad():\n",
        "        outputs = model(new_data_tensor)\n",
        "        if task_type == 'classification':\n",
        "            _, predictions = torch.max(outputs, 1)\n",
        "            predictions = predictions.cpu().numpy()\n",
        "        else:\n",
        "            predictions = outputs.cpu().numpy().flatten()\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# Ejemplo de uso:\n",
        "# predictions = load_model_and_predict('modelo_final.pth', 'scaler.pkl', new_df)\n",
        "print(\"‚úÖ Funci√≥n de carga e inferencia definida\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-WW7SRC9ppd"
      },
      "source": [
        "---\n",
        "\n",
        "## Checklist de Entrega\n",
        "\n",
        "Antes de entregar, verifique que ha completado los siguientes elementos:\n",
        "\n",
        "- [X ] Informaci√≥n del proyecto completada\n",
        "- [X ] Resumen ejecutivo escrito\n",
        "- [X ] Problema de negocio claramente definido\n",
        "- [ X] Objetivos SMART establecidos\n",
        "- [ X] EDA completo con visualizaciones\n",
        "- [ X] Preprocesamiento de datos documentado\n",
        "- [ X] Arquitectura del modelo justificada\n",
        "- [ X] Modelo entrenado con curvas de aprendizaje\n",
        "- [ X] M√©tricas de evaluaci√≥n calculadas\n",
        "- [ X] Comparaci√≥n con modelos baseline\n",
        "- [ X] Interpretaci√≥n de resultados\n",
        "- [ X] Conclusiones y recomendaciones de negocio\n",
        "- [ X] Referencias listadas\n",
        "- [ X] C√≥digo ejecutable sin errores\n",
        "- [ X] Comentarios y documentaci√≥n adecuados\n",
        "\n",
        "---\n",
        "\n",
        "**¬°Buena suerte con su proyecto!** üéì"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HCq-_T6fLH63"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}